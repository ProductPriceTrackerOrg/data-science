{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a5c69f",
   "metadata": {},
   "source": [
    "Here's a cell-wise implementation you can directly copy-paste into your notebook:\n",
    "\n",
    "## Cell 1: Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install tensorflow\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285d38f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 2: Enhanced imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbf8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced imports\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e9086",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 3: Load data (keep your existing data loading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f375c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "camera = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/cameras.csv')\n",
    "laptop = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/laptops.csv')\n",
    "mobile = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/mobile-phones.csv')\n",
    "network_components = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/network-components.csv')\n",
    "perpherials = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/perpherals.csv')\n",
    "smart_watches = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/smart-watches.csv')\n",
    "storage = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/storage.csv')\n",
    "tablets = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/tablets.csv')\n",
    "television = pd.read_csv('/content/data-science/data/raw/price-forecasting/category/television.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03c114",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 4: Process individual datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41889b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each dataset\n",
    "def process_dataset(df, category_name):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['category'] = category_name\n",
    "    df['price'] = df['price'] * 3.4\n",
    "    print(f\"{category_name} - Null values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"{category_name} - Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Process all datasets\n",
    "camera = process_dataset(camera, 'camera')\n",
    "laptop = process_dataset(laptop, 'laptop')\n",
    "mobile = process_dataset(mobile, 'mobile')\n",
    "network_components = process_dataset(network_components, 'network-components')\n",
    "perpherials = process_dataset(perpherials, 'perpherials')\n",
    "smart_watches = process_dataset(smart_watches, 'smart-watches')\n",
    "storage = process_dataset(storage, 'storage')\n",
    "tablets = process_dataset(tablets, 'tablets')\n",
    "television = process_dataset(television, 'television')\n",
    "\n",
    "# Combine all datasets\n",
    "all_products = pd.concat([camera, laptop, mobile, network_components, perpherials, \n",
    "                         smart_watches, storage, tablets, television], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {all_products.shape}\")\n",
    "all_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febacd6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 5: Enhanced data preprocessing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94102b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(df):\n",
    "    \"\"\"Enhanced data preprocessing\"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Convert date column\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Remove outliers using IQR method\n",
    "    Q1 = df['price'].quantile(0.25)\n",
    "    Q3 = df['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    original_size = len(df)\n",
    "    df = df[(df['price'] >= Q1 - 1.5 * IQR) & (df['price'] <= Q3 + 1.5 * IQR)]\n",
    "    print(f\"Removed {original_size - len(df)} outliers\")\n",
    "    \n",
    "    # Sort by product and date\n",
    "    df = df.sort_values(['title', 'brand', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Create unique product ID\n",
    "    df['product_id'] = df.groupby(['title', 'brand']).ngroup()\n",
    "    \n",
    "    # Add price smoothing\n",
    "    print(\"Applying price smoothing...\")\n",
    "    for product_id in df['product_id'].unique():\n",
    "        mask = df['product_id'] == product_id\n",
    "        prices = df.loc[mask, 'price'].values\n",
    "        if len(prices) >= 5:\n",
    "            window_length = min(5, len(prices) if len(prices) % 2 == 1 else len(prices) - 1)\n",
    "            if window_length >= 3:\n",
    "                smoothed = savgol_filter(prices, window_length, 2)\n",
    "                df.loc[mask, 'price_smoothed'] = smoothed\n",
    "            else:\n",
    "                df.loc[mask, 'price_smoothed'] = prices\n",
    "        else:\n",
    "            df.loc[mask, 'price_smoothed'] = prices\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28f139",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 6: Enhanced feature engineering function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_features(df):\n",
    "    \"\"\"Enhanced feature engineering\"\"\"\n",
    "    print(\"Creating advanced features...\")\n",
    "    \n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Enhanced temporal features\n",
    "    df_features['year'] = df_features['date'].dt.year\n",
    "    df_features['month'] = df_features['date'].dt.month\n",
    "    df_features['day'] = df_features['date'].dt.day\n",
    "    df_features['day_of_week'] = df_features['date'].dt.dayofweek\n",
    "    df_features['quarter'] = df_features['date'].dt.quarter\n",
    "    df_features['week_of_year'] = df_features['date'].dt.isocalendar().week\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df_features['is_month_start'] = df_features['date'].dt.is_month_start.astype(int)\n",
    "    df_features['is_month_end'] = df_features['date'].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # Seasonal and holiday features\n",
    "    df_features['is_holiday_season'] = df_features['month'].isin([11, 12]).astype(int)\n",
    "    df_features['is_new_year'] = df_features['month'].eq(1).astype(int)\n",
    "    df_features['is_mid_year'] = df_features['month'].isin([6, 7]).astype(int)\n",
    "    \n",
    "    # Cyclical encoding for temporal features\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day'] / 31)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['day'] / 31)\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b0fe5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 7: Product-specific feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_features(df_features):\n",
    "    \"\"\"Create product-specific features\"\"\"\n",
    "    print(\"Creating product-specific features...\")\n",
    "    \n",
    "    product_features = []\n",
    "    total_products = df_features['product_id'].nunique()\n",
    "    \n",
    "    for idx, product_id in enumerate(df_features['product_id'].unique()):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing product {idx}/{total_products}\")\n",
    "            \n",
    "        product_data = df_features[df_features['product_id'] == product_id].copy()\n",
    "        product_data = product_data.sort_values('date')\n",
    "        \n",
    "        # Enhanced price lags\n",
    "        for lag in [1, 2, 3, 7, 14, 30]:\n",
    "            product_data[f'price_lag_{lag}'] = product_data['price'].shift(lag)\n",
    "        \n",
    "        # Price changes and momentum\n",
    "        for window in [1, 3, 7, 14, 30]:\n",
    "            product_data[f'price_change_{window}d'] = product_data['price'].pct_change(window)\n",
    "            product_data[f'price_momentum_{window}d'] = product_data['price'] / product_data['price'].shift(window) - 1\n",
    "        \n",
    "        # Enhanced rolling statistics\n",
    "        for window in [3, 7, 14, 30]:\n",
    "            product_data[f'price_rolling_mean_{window}'] = product_data['price'].rolling(window, min_periods=1).mean()\n",
    "            product_data[f'price_rolling_std_{window}'] = product_data['price'].rolling(window, min_periods=1).std()\n",
    "            product_data[f'price_rolling_min_{window}'] = product_data['price'].rolling(window, min_periods=1).min()\n",
    "            product_data[f'price_rolling_max_{window}'] = product_data['price'].rolling(window, min_periods=1).max()\n",
    "        \n",
    "        # Price volatility and stability\n",
    "        for window in [7, 14, 30]:\n",
    "            product_data[f'price_volatility_{window}'] = product_data[f'price_change_1d'].rolling(window, min_periods=1).std()\n",
    "        \n",
    "        # Trend analysis\n",
    "        product_data['days_since_start'] = (product_data['date'] - product_data['date'].min()).dt.days\n",
    "        \n",
    "        # Price position features\n",
    "        for window in [7, 14, 30]:\n",
    "            product_data[f'price_percentile_{window}'] = product_data['price'].rolling(window, min_periods=1).rank(pct=True)\n",
    "        \n",
    "        product_features.append(product_data)\n",
    "    \n",
    "    return pd.concat(product_features, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e465bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 8: Market-level features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_market_features(df_features):\n",
    "    \"\"\"Create market-level features\"\"\"\n",
    "    print(\"Creating market-level features...\")\n",
    "    \n",
    "    # Category statistics by date\n",
    "    category_stats = df_features.groupby(['category', 'date'])['price'].agg([\n",
    "        'mean', 'std', 'min', 'max', 'count', 'median'\n",
    "    ]).reset_index()\n",
    "    category_stats.columns = ['category', 'date'] + [f'category_price_{col}' for col in category_stats.columns[2:]]\n",
    "    df_features = df_features.merge(category_stats, on=['category', 'date'], how='left')\n",
    "    \n",
    "    # Brand statistics by date\n",
    "    brand_stats = df_features.groupby(['brand', 'date'])['price'].agg([\n",
    "        'mean', 'std', 'count'\n",
    "    ]).reset_index()\n",
    "    brand_stats.columns = ['brand', 'date'] + [f'brand_price_{col}' for col in brand_stats.columns[2:]]\n",
    "    df_features = df_features.merge(brand_stats, on=['brand', 'date'], how='left')\n",
    "    \n",
    "    # Relative price features\n",
    "    df_features['price_vs_category_mean'] = df_features['price'] / df_features['category_price_mean']\n",
    "    df_features['price_vs_category_median'] = df_features['price'] / df_features['category_price_median']\n",
    "    df_features['price_vs_brand_mean'] = df_features['price'] / df_features['brand_price_mean']\n",
    "    \n",
    "    # Market competition features\n",
    "    df_features['category_competition'] = df_features['category_price_count']\n",
    "    df_features['brand_market_share'] = df_features['brand_price_count'] / df_features['category_price_count']\n",
    "    \n",
    "    # Fill NaN values with forward fill, backward fill, then 0\n",
    "    df_features = df_features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Replace infinite values\n",
    "    df_features = df_features.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d882bf7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 9: Create target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4927986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variables(df):\n",
    "    \"\"\"Create target variables with additional preprocessing\"\"\"\n",
    "    print(\"Creating target variables...\")\n",
    "    \n",
    "    targets = []\n",
    "    \n",
    "    for product_id in df['product_id'].unique():\n",
    "        product_data = df[df['product_id'] == product_id].copy().sort_values('date')\n",
    "        \n",
    "        # Create 7-day ahead targets\n",
    "        for i in range(1, 8):\n",
    "            product_data[f'price_target_{i}d'] = product_data['price'].shift(-i)\n",
    "        \n",
    "        targets.append(product_data)\n",
    "    \n",
    "    df_with_targets = pd.concat(targets, ignore_index=True)\n",
    "    \n",
    "    # Remove rows where we don't have all 7-day targets\n",
    "    target_cols = [f'price_target_{i}d' for i in range(1, 8)]\n",
    "    df_with_targets = df_with_targets.dropna(subset=target_cols)\n",
    "    \n",
    "    return df_with_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd5263",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 10: Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(df):\n",
    "    \"\"\"Enhanced data preparation\"\"\"\n",
    "    print(\"Preparing model data...\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_brand = LabelEncoder()\n",
    "    le_category = LabelEncoder()\n",
    "    \n",
    "    df['brand_encoded'] = le_brand.fit_transform(df['brand'])\n",
    "    df['category_encoded'] = le_category.fit_transform(df['category'])\n",
    "    \n",
    "    # Select features (exclude target columns and non-feature columns)\n",
    "    exclude_cols = ['title', 'brand', 'category', 'date', 'price', 'price_smoothed'] + \\\n",
    "                  [f'price_target_{i}d' for i in range(1, 8)]\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    target_cols = [f'price_target_{i}d' for i in range(1, 8)]\n",
    "    \n",
    "    print(f\"Number of features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    y = df[target_cols].fillna(0)\n",
    "    \n",
    "    # Use RobustScaler for better outlier handling\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y.values, df, scaler, le_brand, le_category, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf1101",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 11: Execute preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73399fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the preprocessing pipeline\n",
    "print(\"Starting preprocessing pipeline...\")\n",
    "\n",
    "# Step 1: Basic preprocessing\n",
    "df = load_and_preprocess_data(all_products)\n",
    "\n",
    "# Step 2: Create temporal features\n",
    "df = create_advanced_features(df)\n",
    "\n",
    "# Step 3: Create product-specific features\n",
    "df = create_product_features(df)\n",
    "\n",
    "# Step 4: Create market-level features\n",
    "df = create_market_features(df)\n",
    "\n",
    "# Step 5: Create target variables\n",
    "df = create_target_variables(df)\n",
    "\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a2081",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 12: Prepare data for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ab77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data\n",
    "X, y, df_processed, scaler, le_brand, le_category, feature_cols = prepare_model_data(df)\n",
    "\n",
    "# Time-based split (use more recent data for validation)\n",
    "split_date = df_processed['date'].quantile(0.85)\n",
    "train_mask = df_processed['date'] <= split_date\n",
    "\n",
    "X_train, X_val = X[train_mask], X[~train_mask]\n",
    "y_train, y_val = y[train_mask], y[~train_mask]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b316aa7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 13: Build LightGBM models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc282b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightgbm_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Build LightGBM models\"\"\"\n",
    "    print(\"Building LightGBM models...\")\n",
    "    \n",
    "    lgb_models = []\n",
    "    for i in range(7):\n",
    "        print(f\"Training LightGBM for day {i+1}...\")\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train[:, i],\n",
    "            eval_set=[(X_val, y_val[:, i])],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        lgb_models.append(model)\n",
    "    \n",
    "    return lgb_models\n",
    "\n",
    "# Train LightGBM models\n",
    "lgb_models = build_lightgbm_models(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23392646",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 14: Build XGBoost models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgboost_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Build XGBoost models\"\"\"\n",
    "    print(\"Building XGBoost models...\")\n",
    "    \n",
    "    xgb_models = []\n",
    "    for i in range(7):\n",
    "        print(f\"Training XGBoost for day {i+1}...\")\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train[:, i],\n",
    "            eval_set=[(X_val, y_val[:, i])],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        xgb_models.append(model)\n",
    "    \n",
    "    return xgb_models\n",
    "\n",
    "# Train XGBoost models\n",
    "xgb_models = build_xgboost_models(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c648d62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 15: Build Neural Network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Build enhanced neural network model\"\"\"\n",
    "    print(\"Building Neural Network model...\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(7)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='huber',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(patience=10, factor=0.5)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train Neural Network\n",
    "nn_model, history = build_neural_network(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e5be9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 16: Build ensemble predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d85ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(X, lgb_models, xgb_models, nn_model):\n",
    "    \"\"\"Make ensemble predictions with dynamic weighting\"\"\"\n",
    "    print(\"Making ensemble predictions...\")\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    lgb_preds = np.array([model.predict(X) for model in lgb_models]).T\n",
    "    xgb_preds = np.array([model.predict(X) for model in xgb_models]).T\n",
    "    nn_preds = nn_model.predict(X, verbose=0)\n",
    "    \n",
    "    # Dynamic ensemble with different weights for different prediction horizons\n",
    "    weights = {\n",
    "        'lightgbm': [0.4, 0.35, 0.35, 0.3, 0.3, 0.25, 0.25],\n",
    "        'xgboost': [0.35, 0.35, 0.3, 0.3, 0.25, 0.25, 0.2],\n",
    "        'neural_network': [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55]\n",
    "    }\n",
    "    \n",
    "    ensemble_pred = np.zeros_like(lgb_preds)\n",
    "    for i in range(7):\n",
    "        ensemble_pred[:, i] = (\n",
    "            weights['lightgbm'][i] * lgb_preds[:, i] +\n",
    "            weights['xgboost'][i] * xgb_preds[:, i] +\n",
    "            weights['neural_network'][i] * nn_preds[:, i]\n",
    "        )\n",
    "    \n",
    "    return ensemble_pred, {'lightgbm': lgb_preds, 'xgboost': xgb_preds, 'neural_network': nn_preds}\n",
    "\n",
    "# Make ensemble predictions\n",
    "ensemble_pred, individual_preds = predict_ensemble(X_val, lgb_models, xgb_models, nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb29e4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 17: Enhanced evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a81b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_enhanced(y_true, y_pred, model_name=\"Ensemble\"):\n",
    "    \"\"\"Enhanced model evaluation\"\"\"\n",
    "    print(f\"\\n--- {model_name} Model Performance Evaluation ---\")\n",
    "    \n",
    "    metrics = {}\n",
    "    for i in range(7):\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        \n",
    "        # Calculate MAPE manually to handle division by zero\n",
    "        mape = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / np.maximum(y_true[:, i], 1e-8))) * 100\n",
    "        \n",
    "        metrics[f'day_{i+1}'] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nMetrics for Day {i+1}:\")\n",
    "        print(f\"  MAE:  {mae:.2f}\")\n",
    "        print(f\"  RMSE: {rmse:.2f}\")\n",
    "        print(f\"  R²:   {r2:.4f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mae = mean_absolute_error(y_true, y_pred)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    overall_r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n--- Overall Performance ---\")\n",
    "    print(f\"  Overall MAE:  {overall_mae:.2f}\")\n",
    "    print(f\"  Overall RMSE: {overall_rmse:.2f}\")\n",
    "    print(f\"  Overall R²:   {overall_r2:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate ensemble model\n",
    "ensemble_metrics = evaluate_model_enhanced(y_val, ensemble_pred, \"Ensemble\")\n",
    "\n",
    "# Evaluate individual models\n",
    "lgb_metrics = evaluate_model_enhanced(y_val, individual_preds['lightgbm'], \"LightGBM\")\n",
    "xgb_metrics = evaluate_model_enhanced(y_val, individual_preds['xgboost'], \"XGBoost\")\n",
    "nn_metrics = evaluate_model_enhanced(y_val, individual_preds['neural_network'], \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51ec5f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 18: Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(y_true, ensemble_pred, individual_preds, day_to_plot=0, num_samples=100):\n",
    "    \"\"\"Plot model comparison\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    models = ['lightgbm', 'xgboost', 'neural_network']\n",
    "    model_names = ['LightGBM', 'XGBoost', 'Neural Network']\n",
    "    \n",
    "    sample_indices = np.random.choice(len(y_true), min(num_samples, len(y_true)), replace=False)\n",
    "    \n",
    "    # Individual models\n",
    "    for i, (model_key, model_name) in enumerate(zip(models, model_names)):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(y_true[sample_indices, day_to_plot], \n",
    "                  individual_preds[model_key][sample_indices, day_to_plot], \n",
    "                  alpha=0.6, s=30)\n",
    "        ax.plot([y_true[:, day_to_plot].min(), y_true[:, day_to_plot].max()],\n",
    "               [y_true[:, day_to_plot].min(), y_true[:, day_to_plot].max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual Price')\n",
    "        ax.set_ylabel('Predicted Price')\n",
    "        ax.set_title(f'{model_name} - Day {day_to_plot + 1}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ensemble model\n",
    "    ax = axes[3]\n",
    "    ax.scatter(y_true[sample_indices, day_to_plot], \n",
    "              ensemble_pred[sample_indices, day_to_plot], \n",
    "              alpha=0.6, s=30, color='purple')\n",
    "    ax.plot([y_true[:, day_to_plot].min(), y_true[:, day_to_plot].max()],\n",
    "           [y_true[:, day_to_plot].min(), y_true[:, day_to_plot].max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual Price')\n",
    "    ax.set_ylabel('Predicted Price')\n",
    "    ax.set_title(f'Ensemble Model - Day {day_to_plot + 1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison for Day 1\n",
    "plot_model_comparison(y_val, ensemble_pred, individual_preds, day_to_plot=0, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2f82f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 19: Feature importance analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(lgb_models, xgb_models, feature_cols):\n",
    "    \"\"\"Analyze feature importance\"\"\"\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    \n",
    "    # Get feature importance from LightGBM (Day 1 model)\n",
    "    lgb_importance = lgb_models[0].feature_importances_\n",
    "    \n",
    "    # Get feature importance from XGBoost (Day 1 model)\n",
    "    xgb_importance = xgb_models[0].feature_importances_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'lgb_importance': lgb_importance,\n",
    "        'xgb_importance': xgb_importance\n",
    "    })\n",
    "    \n",
    "    # Calculate average importance\n",
    "    importance_df['avg_importance'] = (importance_df['lgb_importance'] + importance_df['xgb_importance']) / 2\n",
    "    importance_df = importance_df.sort_values('avg_importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(20)\n",
    "    \n",
    "    x = np.arange(len(top_features))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, top_features['lgb_importance'], width, label='LightGBM', alpha=0.8)\n",
    "    plt.bar(x + width/2, top_features['xgb_importance'], width, label='XGBoost', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Top 20 Feature Importance Comparison')\n",
    "    plt.xticks(x, top_features['feature'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_df = analyze_feature_importance(lgb_models, xgb_models, feature_cols)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df[['feature', 'avg_importance']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529207cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Cell 20: Save models and results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamp for model versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save models and preprocessing objects\n",
    "models_to_save = {\n",
    "    'lgb_models': lgb_models,\n",
    "    'xgb_models': xgb_models,\n",
    "    'nn_model': nn_model,\n",
    "    'scaler': scaler,\n",
    "    'le_brand': le_brand,\n",
    "    'le_category': le_category,\n",
    "    'feature_cols': feature_cols,\n",
    "    'ensemble_metrics': ensemble_metrics,\n",
    "    'importance_df': importance_df\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "model_filename = f'price_forecasting_models_{timestamp}.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(models_to_save, f)\n",
    "\n",
    "print(f\"Models saved to: {model_filename}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset size: {df_processed.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Ensemble R² Score: {ensemble_metrics['day_1']['R2']:.4f}\")\n",
    "print(f\"Ensemble MAE: {ensemble_metrics['day_1']['MAE']:.2f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f51eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This implementation provides a complete, enhanced price forecasting model with:\n",
    "\n",
    "1. **Better data preprocessing** with outlier removal and price smoothing\n",
    "2. **Advanced feature engineering** with 50+ features\n",
    "3. **Ensemble modeling** with LightGBM, XGBoost, and Neural Networks\n",
    "4. **Dynamic weighting** based on prediction horizon\n",
    "5. **Comprehensive evaluation** and visualization\n",
    "6. **Feature importance analysis**\n",
    "7. **Model persistence** for deployment\n",
    "\n",
    "The model should achieve significantly better accuracy than your current implementation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
